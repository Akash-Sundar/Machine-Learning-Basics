{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[{"file_id":"1Avj78p4e-UpToQPTOSZpLvz4amBKiEjh","timestamp":1666739672239},{"file_id":"1LyFRSMtOxWFZzd2L1C2bXZpujlYQhBqC","timestamp":1666738117435},{"file_id":"1D_rq09CQe_ZffYCIkbIaZcddM2y4DJAR","timestamp":1634141946466}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"VuUMnYFVf57O"},"source":["# **CIS 520: Machine Learning**\n","\n","## **PCA**\n","\n","\n","- **Content Creator:** Hanwen Zhang, Siyun Hu\n","- **Content Reviewers:** Xiayan Ji\n","- **Reference:**\n","https://github.com/jakevdp/PythonDataScienceHandbook\n","https://etav.github.io/python/scikit_pca.html\n","https://aiaspirant.com/types-of-pca/\n","- **Objectives:** The worksheet aims to:\n","  - Help you understand PCA from the geometric meaning\n","  - Help you understand common applications of PCA, including dimensionality reduction, visualization and noise filtering.\n","  - Understand reconstruction error\n","  - Explore real-world example of PCA (e.g. eigenfaces)"]},{"cell_type":"code","metadata":{"id":"y-spyTMPf57R"},"source":["import sys\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","import os\n","import seaborn as sns\n","sns.set()\n","from sklearn import decomposition\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import scale\n","import requests\n","import io\n","\n","# to make this notebook's output stable across runs\n","np.random.seed(42)\n","\n","# To plot pretty figures\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tn3wuswcj3fw"},"source":["## **Autograding and the PennGrader**\n","\n","\n","Enter your PennID (numbers not letters!) in the specified section."]},{"cell_type":"code","metadata":{"id":"AFixFKy2kAev"},"source":["%%capture\n","!pip install penngrader\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random \n","import numpy as np\n","import pandas as pd\n","import os\n","import sys\n","import matplotlib.pyplot as plt\n","from numpy.linalg import *\n","\n","import dill\n","import base64"],"metadata":{"id":"HFBgYui-WnCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For autograder only, do not modify this cell. \n","# True for Google Colab, False for autograder\n","NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n","if NOTEBOOK:\n","    print(\"[INFO, OK] Google Colab.\")\n","else:\n","    print(\"[INFO, OK] Autograder.\")\n","    sys.exit()"],"metadata":{"id":"Wql2sB6dW6x-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Insert PennID here!"],"metadata":{"id":"3f1c5R-ZXCwC"}},{"cell_type":"code","metadata":{"id":"kcS79C9blJBm"},"source":["#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO \n","#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n","STUDENT_ID = 99999999 # YOUR PENN-ID GOES HERE AS AN INTEGER#"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wd0oMe8glNnA"},"source":["import penngrader.grader\n","\n","grader = penngrader.grader.PennGrader(homework_id = 'CIS_5200_202230_HW_PCA_WS', student_id = STUDENT_ID)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A helper function for grading utils\n","def grader_serialize(obj):        # A helper function\n","    '''Dill serializes Python object into a UTF-8 string'''\n","    byte_serialized = dill.dumps(obj, recurse = True)\n","    return base64.b64encode(byte_serialized).decode(\"utf-8\")"],"metadata":{"id":"oxdWoARsXS-0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"UlkzIee8qEjo"},"source":["## **Introduction of PCA**\n","\n","*Principal component analysis (PCA)* is a fast and flexible unsupervised method for dimensionality reduction in data, which we saw briefly in [Introducing Scikit-Learn](05.02-Introducing-Scikit-Learn.ipynb).\n","Its behavior is easiest to visualize by looking at a two-dimensional dataset.\n","Consider the following 200 points:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"p2bSjOEpqEjp"},"source":["rng = np.random.RandomState(1)\n","X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n","plt.scatter(X[:, 0], X[:, 1])\n","plt.axis('equal');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"mjYhDGaaqEju"},"source":["By eye, it is clear that there is a nearly linear relationship between the x and y variables.\n","This is reminiscent of the linear regression data we explored in [In Depth: Linear Regression](05.06-Linear-Regression.ipynb), but the problem setting here is slightly different: rather than attempting to *predict* the y values from the x values, the unsupervised learning problem attempts to learn about the *relationship* between the x and y values.\n","\n","In principal component analysis, this relationship is quantified by finding a list of the *principal axes* in the data, and using those axes to describe the dataset.\n","Using Scikit-Learn's ``PCA`` estimator, we can compute this as follows:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"KPDll1-NqEjv"},"source":["pca = PCA(n_components=2)\n","pca.fit(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"cjqGZKq3qEjx"},"source":["### Component and loadings\n","The fit learns some quantities from the data, most importantly the \"components\" and \"explained variance\":"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"aothWUVuqEjy"},"source":["print(pca.components_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"VlPcGqE6qEj0"},"source":["print(pca.explained_variance_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"RhINhPAzqEj3"},"source":["To see what these numbers mean, let's visualize them as vectors over the input data, using the \"components\" to define the direction of the vector, and the \"explained variance\" to define the squared-length of the vector:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"AdZDGT6aqEj3"},"source":["def draw_vector(v0, v1, ax=None):\n","    ax = ax or plt.gca()\n","    arrowprops=dict(arrowstyle='->',\n","                    linewidth=2,\n","                    shrinkA=0, shrinkB=0)\n","    ax.annotate('', v1, v0, arrowprops=arrowprops)\n","\n","# plot data\n","plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n","for length, vector in zip(pca.explained_variance_, pca.components_):\n","    v = vector * 3 * np.sqrt(length)\n","    draw_vector(pca.mean_, pca.mean_ + v)\n","plt.axis('equal');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"7pxZB_0QqEj8"},"source":["These vectors represent the *principal axes* of the data, and the length of the vector is an indication of how \"important\" that axis is in describing the distribution of the data—more precisely, it is a measure of the variance of the data when projected onto that axis.\n","The projection of each data point onto the principal axes are the \"principal components\" of the data.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vZmsajv1OkEk"},"source":["## *Question 1*\n","\n","<!-- Now, you know the geometric meaning of components and explained variance. But what's their relationship with eigenvalues and eigenvectors? Explain it with mathematics.  -->\n","\n","*Hint: course wiki.*"]},{"cell_type":"code","metadata":{"id":"BgvRDlnwdMAn"},"source":["#@markdown Now, you know the geometric meaning of components and explained variance. But what's their relationship with eigenvalues and eigenvectors? Explain it with mathematics. \n","Q1 = '' #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grader.grade(test_case_id = 'eigenvalues', answer = Q1)"],"metadata":{"id":"IQEOKKeSZAiL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eeUYvxSmziyk"},"source":["### Maximize Variance\n","\n","PCA allows us to quantify the trade-offs between the number of features we utilize and the total variance explained by the data. PCA allows us to determine which features capture similiar information and discard them to create a more parsimonious model.\n","\n","In order to perform PCA we need to do the following:\n","1. Standardize the data.\n","2. Use the standardized data to create a covariance matrix.\n","3. Use the resulting matrix to calculate eigenvectors (principal components) and their corresponding eigenvalues.\n","4. Sort the components in decending order by its eigenvalue.\n","5. Choose n components which explain the most variance within the data (larger eigenvalue means the feature explains more variance).\n","6. Create a new matrix using the n components.\n","\n","NOTE: PCA compresses the feature space so you will not be able to tell which variables explain the most variance because they have been transformed. If you'd like to preserve the original features to determine which ones explain the most variance for a given data set, see the [SciKit Learn Feature Documentation.](https://scikit-learn.org/stable/modules/feature_selection.html)\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"_H0Vep1S02Fr"},"source":["sns.set(font_scale=1.2,style=\"whitegrid\") #set styling preferences\n","\n","from sklearn.datasets import load_digits\n","digits = load_digits()\n","digits.data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HQUi0yug1HKM"},"source":["# Standardize the Dataset\n","# convert the data into a numpy array\n","x = digits.data \n","x = scale(x)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wErhcc97JtU"},"source":["#Create a Covariance Matrix\n","covar_matrix = PCA(n_components = 64) #we have 64 features\n","\n","covar_matrix.fit(x)\n","variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n","\n","var=np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3)*100)\n","var #cumulative sum of variance explained with [n] features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KTJoEhVM8SkC"},"source":["In the above array we see that the first feature explains roughly 12% of the variance within our data set while the first two explain 21.6% and so on. If we employ 49 features we capture 98.1% of the variance within the dataset, thus we gain very little by implementing an additional feature (think of this as diminishing marginal return on total variance explained)."]},{"cell_type":"code","metadata":{"id":"8tdRUyMa8hrC"},"source":["plt.ylabel('% Variance Explained')\n","plt.xlabel('# of Features')\n","plt.title('PCA Analysis')\n","plt.ylim(30,100.5)\n","plt.style.context('seaborn-whitegrid')\n","\n","\n","plt.plot(var)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UZ02Cjc98mHA"},"source":["## *Question 2*\n","\n","<!-- Based on the plot above, how many features shall we take? And why? -->\n","\n"]},{"cell_type":"code","metadata":{"id":"KgJ-4HMD6Pdm"},"source":["#@markdown Based on the plot above, how many features shall we take? And why? (Don't need an exact numeric answer here)\n","Q2 = '' #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grader.grade(test_case_id = 'num_features', answer = Q2)"],"metadata":{"id":"aXJFqHERZEu1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q8WgY3vWPMyP"},"source":["## **Applications of PCA**\n"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"ns15EKOlqEj9"},"source":["### Dimensionality reduction\n","\n","Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n","\n","Here is an example of using PCA as a dimensionality reduction transform:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"DzrkrgLmqEj-"},"source":["pca = PCA(n_components=1)\n","pca.fit(X)\n","X_pca = pca.transform(X)\n","print(\"original shape:   \", X.shape)\n","print(\"transformed shape:\", X_pca.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"NlFeCEDfqEkA"},"source":["The transformed data has been reduced to a single dimension.\n","To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"o3gTSEeVqEkB"},"source":["X_new = pca.inverse_transform(X_pca)\n","plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n","plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n","plt.axis('equal');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"6q6IvsICqEkE"},"source":["The blue points are the original data, while the orange points are the projected version.\n","This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance.\n","The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n","\n","This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved."]},{"cell_type":"code","metadata":{"id":"GEPO3_dW7VLr"},"source":["#This calculates the reconstruction error.\n","np.sum((X - X_new) ** 2, axis=1).mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"wEU40ME7qEkF"},"source":["### Visualize high-dimensional data\n","\n","The usefulness of the dimensionality reduction may not be entirely apparent in only two dimensions, but becomes much more clear when looking at high-dimensional data.\n","To see this, let's take a quick look at the application of PCA to the digits data we saw in [In-Depth: Decision Trees and Random Forests](05.08-Random-Forests.ipynb).\n","\n","We start by loading the data:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"8zs7ddvkqEkF"},"source":["from sklearn.datasets import load_digits\n","digits = load_digits()\n","digits.data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"anK1T6XFqEkI"},"source":["Recall that the data consists of 8×8 pixel images, meaning that they are 64-dimensional.\n","To gain some intuition into the relationships between these points, we can use PCA to project them to a more manageable number of dimensions, say two:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"UX4Cnwl6qEkI"},"source":["pca = PCA(2)  # project from 64 to 2 dimensions\n","projected = pca.fit_transform(digits.data)\n","print(digits.data.shape)\n","print(projected.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"rMEKOA9oqEkL"},"source":["We can now plot the first two principal components of each point to learn about the data:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"lC0dO3BaqEkM"},"source":["plt.scatter(projected[:, 0], projected[:, 1],\n","            c=digits.target, edgecolor='none', alpha=0.5,\n","            cmap=plt.cm.get_cmap('cool', 10))\n","plt.xlabel('component 1')\n","plt.ylabel('component 2')\n","plt.colorbar();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"-AQ3cHBuqEkP"},"source":["Recall what these components mean: the full data is a 64-dimensional point cloud, and these points are the projection of each data point along the directions with the largest variance.\n","Essentially, we have found the optimal stretch and rotation in 64-dimensional space that allows us to see the layout of the digits in two dimensions, and have done this in an unsupervised manner—that is, without reference to the labels."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"KMBOtVffqEkX"},"source":["### Noise Filtering\n","\n","PCA can also be used as a filtering approach for noisy data.\n","The idea is this: any components with variance much larger than the effect of the noise should be relatively unaffected by the noise.\n","So if you reconstruct the data using just the largest subset of principal components, you should be preferentially keeping the signal and throwing out the noise.\n","\n","Let's see how this looks with the digits data.\n","First we will plot several of the input noise-free data:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"gxnXOakUqEkX"},"source":["def plot_digits(data):\n","    fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n","                             subplot_kw={'xticks':[], 'yticks':[]},\n","                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n","    for i, ax in enumerate(axes.flat):\n","        ax.imshow(data[i].reshape(8, 8),\n","                  cmap='binary', interpolation='nearest',\n","                  clim=(0, 16))\n","plot_digits(digits.data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"8-Ni5UOyqEkb"},"source":["Now lets add some random noise to create a noisy dataset, and re-plot it:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"wH4yfeEVqEkd"},"source":["np.random.seed(42)\n","noisy = np.random.normal(digits.data, 4)\n","plot_digits(noisy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"do7XY0WvqEkf"},"source":["It's clear by eye that the images are noisy, and contain spurious pixels.\n","Let's train a PCA on the noisy data, requesting that the projection preserve 50% of the variance:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"OeX1H3VnqEkf"},"source":["pca = PCA(0.50).fit(noisy)\n","pca.n_components_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"NfbNiMAqqEkh"},"source":["Here 50% of the variance amounts to 12 principal components.\n","Now we compute these components, and then use the inverse of the transform to reconstruct the filtered digits:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"p72xVUNgqEki"},"source":["components = pca.transform(noisy)\n","filtered = pca.inverse_transform(components)\n","plot_digits(filtered)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"u8SAvtRDqEkk"},"source":["This signal preserving/noise filtering property makes PCA a very useful feature selection routine—for example, rather than training a classifier on very high-dimensional data, you might instead train the classifier on the lower-dimensional representation, which will automatically serve to filter out random noise in the inputs."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"7AdJM8D9qEkk"},"source":["## Example: Eigenfaces\n","\n","Earlier we explored an example of using a PCA projection as a feature selector for facial recognition with a support vector machine (see [In-Depth: Support Vector Machines](05.07-Support-Vector-Machines.ipynb)).\n","Here we will take a look back and explore a bit more of what went into that.\n","Recall that we were using the Labeled Faces in the Wild dataset made available through Scikit-Learn:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"OzHmhL8UqEkl"},"source":["from sklearn.datasets import fetch_lfw_people\n","faces = fetch_lfw_people(min_faces_per_person=60)\n","print(faces.target_names)\n","print(faces.images.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"95m18lq9qEkn"},"source":["Let's take a look at the principal axes that span this dataset.\n"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"N3YWyESXqEkn"},"source":["pca = PCA(150)\n","pca.fit(faces.data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"mrhzmh3kqEkq"},"source":["In this case, it can be interesting to visualize the images associated with the first several principal components (these components are technically known as \"eigenvectors,\"\n","so these types of images are often called \"eigenfaces\").\n","As you can see in this figure, they are as creepy as they sound:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"68dhR5W6qEkr"},"source":["fig, axes = plt.subplots(3, 8, figsize=(9, 4),\n","                         subplot_kw={'xticks':[], 'yticks':[]},\n","                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Cgl06WFsqEku"},"source":["The results are very interesting, and give us insight into how the images vary: for example, the first few eigenfaces (from the top left) seem to be associated with the angle of lighting on the face, and later principal vectors seem to be picking out certain features, such as eyes, noses, and lips.\n","Let's take a look at the cumulative variance of these components to see how much of the data information the projection is preserving:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"5QlPMssXqEkv"},"source":["plt.plot(np.cumsum(pca.explained_variance_ratio_))\n","plt.xlabel('number of components')\n","plt.ylabel('cumulative explained variance');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"wcwiteE6qEkx"},"source":["We see that these 150 components account for just over 90% of the variance.\n","That would lead us to believe that using these 150 components, we would recover most of the essential characteristics of the data.\n","To make this more concrete, we can compare the input images with the images reconstructed from these 150 components:"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"vlONDQoxqEkx"},"source":["# Compute the components and projected faces\n","pca = PCA(150).fit(faces.data)\n","components = pca.transform(faces.data)\n","projected = pca.inverse_transform(components)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"QpCqzUmCqEkz"},"source":["# Plot the results\n","fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),\n","                       subplot_kw={'xticks':[], 'yticks':[]},\n","                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n","for i in range(10):\n","    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n","    ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')\n","    \n","ax[0, 0].set_ylabel('full-dim\\ninput')\n","ax[1, 0].set_ylabel('150-dim\\nreconstruction');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"TMze7YFxqEk1"},"source":["The top row here shows the input images, while the bottom row shows the reconstruction of the images from just 150 of the ~3,000 initial features.\n","This visualization makes clear why the PCA feature selection used in [In-Depth: Support Vector Machines](05.07-Support-Vector-Machines.ipynb) was so successful: although it reduces the dimensionality of the data by nearly a factor of 20, the projected images contain enough information that we might, by eye, recognize the individuals in the image.\n","What this means is that our classification algorithm needs to be trained on 150-dimensional data rather than 3,000-dimensional data, which depending on the particular algorithm we choose, can lead to a much more efficient classification."]},{"cell_type":"markdown","metadata":{"id":"8M2H8McLFV1E"},"source":["## **Summary**\n","\n","In this section we have discussed the use of principal component analysis for dimensionality reduction, for visualization of high-dimensional data, for noise filtering. Because of the versatility and interpretability of PCA, it has been shown to be effective in a wide variety of contexts and disciplines.\n","\n","Given any high-dimensional dataset, I tend to start with PCA in order to visualize the relationship between points (as we did with the digits), to understand the main variance in the data (as we did with the eigenfaces), and to understand the intrinsic dimensionality (by plotting the explained variance ratio).\n","\n","Certainly PCA is not useful for every high-dimensional dataset, but it offers a straightforward and efficient path to gaining insight into high-dimensional data.\n","\n","PCA's main weakness is that it tends to be highly affected by outliers in the data. For this reason, many robust variants of PCA have been developed, many of which act to iteratively discard data points that are poorly described by the initial components. Scikit-Learn contains a couple interesting variants on PCA, such as ``RandomizedPCA`` and ``SparsePCA``, both also in the ``sklearn.decomposition`` submodule. You could explore the documentation of these methods if you are interested.\n"]},{"cell_type":"markdown","source":["\n","## Submission\n","\n","Please make sure to download your finished worksheet as .ipynb and again as .py, and submit both PCA_WS.ipynb and PCA_WS.py to Gradescope.\n"],"metadata":{"id":"T3tqq64aaHnj"}}]}